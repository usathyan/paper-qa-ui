# Paper-QA UI

A modern Gradio web interface for [Paper-QA](https://github.com/Future-House/paper-qa), enabling high-accuracy Retrieval Augmented Generation (RAG) on scientific documents with citations.

## ğŸš€ Features

- **ğŸ“š Document Upload**: Upload PDF papers and automatically index them
- **â“ Question Interface**: Ask questions about uploaded documents
- **ğŸ“ Answer Display**: Get formatted answers with proper citations
- **ğŸ“š Evidence Sources**: See which parts of documents were used
- **âš™ï¸ Configuration**: Choose between different model configurations
- **ğŸ”„ Real-time Status**: Live updates during processing
- **ğŸ§¹ Clear Functionality**: Reset and start fresh

## ğŸ¯ Quick Start

### Prerequisites

- Python 3.11+
- [Ollama](https://ollama.com/) installed and running
- Optional: OpenRouter API key for cloud LLMs

### Installation

1. **Clone and setup**:
   ```bash
   git clone <repository-url>
   cd paper-qa-ui
   make setup
   ```

2. **Configure environment** (optional):
   ```bash
   # Edit .env file to add your OpenRouter API key
   cp env.template .env
   # Edit .env and add: OPENROUTER_API_KEY=your_key_here
   ```

3. **Start the UI**:
   ```bash
   make ui
   ```

4. **Access the interface**:
   - Open your browser to: http://localhost:7860
   - Upload PDF documents
   - Ask questions about the documents

## ğŸ› ï¸ Usage

### Web Interface

1. **Upload Documents**: Use the file upload to add PDF papers
2. **Process Documents**: Click "Process Documents" to index them
3. **Ask Questions**: Enter questions about the uploaded papers
4. **View Results**: See answers, sources, and processing information

### Configuration Options

The UI supports multiple configurations:

- **`optimized_ollama`** (Default): Local Ollama with performance tuning
- **`openrouter_ollama`**: OpenRouter LLM + Ollama embeddings
- **`ollama`**: Pure local Ollama setup
- **`clinical_trials`**: Includes clinical trials search capability

### CLI Usage

For command-line usage:

```bash
# Test CLI functionality
make test-cli

# Run CLI example
make cli-example

# Direct CLI usage
python scripts/paper_qa_cli.py "What is this paper about?"
```

## ğŸ§ª Testing

Run comprehensive tests:

```bash
# Test UI functionality
make test-ui-functionality

# Test file upload
make test-file-upload

# Test complete workflow
make test-complete-workflow

# Test CLI
make test-cli
```

## ğŸ“ Project Structure

```
paper-qa-ui/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ paperqa2_ui.py      # Main Gradio UI application
â”‚   â”œâ”€â”€ config_manager.py   # Configuration management
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ configs/                 # Configuration files
â”‚   â”œâ”€â”€ optimized_ollama.json
â”‚   â”œâ”€â”€ openrouter_ollama.json
â”‚   â”œâ”€â”€ ollama.json
â”‚   â””â”€â”€ clinical_trials.json
â”œâ”€â”€ scripts/                # Utility scripts
â”‚   â”œâ”€â”€ run_gradio_ui.py
â”‚   â”œâ”€â”€ paper_qa_cli.py
â”‚   â””â”€â”€ test_*.py
â”œâ”€â”€ papers/                 # Uploaded documents
â”œâ”€â”€ indexes/                # Paper-QA indexes
â””â”€â”€ Makefile               # Build and management commands
```

## âš™ï¸ Configuration

### Optimized Ollama Configuration

The default configuration (`optimized_ollama`) is tuned for performance:

- **LLM**: `ollama/llama3.2`
- **Embedding**: `ollama/nomic-embed-text`
- **Evidence K**: 20
- **Max Sources**: 7
- **Concurrent Requests**: 1
- **Temperature**: 0.2

### Environment Variables

- `OPENROUTER_API_KEY`: Your OpenRouter API key (optional)

## ğŸ”§ Development

### Setup Development Environment

```bash
make setup
```

### Run Tests

```bash
make test-ui-functionality
make test-file-upload
make test-complete-workflow
```

### Clean Up

```bash
make clean-data      # Clean session data
make clean-all-data  # Clean all data including papers
make kill-server     # Kill hanging server processes
```

## ğŸ› Troubleshooting

### Common Issues

1. **Port 7860 in use**:
   ```bash
   make kill-server
   make ui
   ```

2. **Ollama not running**:
   ```bash
   ollama serve
   ```

3. **Missing models**:
   ```bash
   ollama pull llama3.2
   ollama pull nomic-embed-text
   ```

4. **File upload errors**: The UI now properly handles Gradio file objects and duplicate files.

### Logs

Check the terminal output for detailed logs. The UI provides real-time status updates during processing.

## ğŸ“Š Performance

The optimized configuration provides:
- **Fast indexing**: Efficient document processing
- **Accurate retrieval**: 20 evidence sources with relevance scoring
- **Local processing**: No external API calls required (with Ollama)
- **Real-time feedback**: Live status updates

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests: `make test-ui-functionality`
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Paper-QA](https://github.com/Future-House/paper-qa) - The core RAG engine
- [Ollama](https://ollama.com/) - Local LLM runner
- [Gradio](https://gradio.app/) - Web interface framework
- [LiteLLM](https://github.com/BerriAI/litellm) - LLM abstraction layer